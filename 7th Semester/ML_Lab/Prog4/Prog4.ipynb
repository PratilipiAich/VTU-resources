{"metadata":{"language_info":{"name":"python","version":"3.7.8","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nX = np.array(([2, 9], [1, 5], [3, 6]), dtype=float) # two inputs [sleep,study]\ny = np.array(([92], [86], [89]), dtype=float) # one output [Expected % in Exams]\nX = X/np.amax(X,axis=0) # maximum of X array longitudinally\ny = y/100","metadata":{"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"#Sigmoid Function\ndef sigmoid (x):\n    return 1/(1 + np.exp(-x))","metadata":{"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"#Derivative of Sigmoid Function\ndef derivatives_sigmoid(x):\n    return x * (1 - x)","metadata":{"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"#Variable initialization\nepoch=5000 \t#Setting training iterations\nlr=0.1 \t\t#Setting learning rate\ninputlayer_neurons = 2 \t\t#number of features in data set\nhiddenlayer_neurons = 3 \t#number of hidden layers neurons\noutput_neurons = 1 \t\t#number of neurons at output layer","metadata":{"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"\n#weight and bias initialization\nwh=np.random.uniform(size=(inputlayer_neurons,hiddenlayer_neurons)) #weight of the link from input node to hidden node\nbh=np.random.uniform(size=(1,hiddenlayer_neurons)) # bias of the link from input node to hidden node\nwout=np.random.uniform(size=(hiddenlayer_neurons,output_neurons)) #weight of the link from hidden node to output node\nbout=np.random.uniform(size=(1,output_neurons)) #bias of the link from hidden node to output node","metadata":{"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"#draws a random range of numbers uniformly of dim x*y\nfor i in range(epoch):\n\n#Forward Propogation\n    hinp1=np.dot(X,wh)\n    hinp=hinp1 + bh\n    hlayer_act = sigmoid(hinp)\n    outinp1=np.dot(hlayer_act,wout)\n    outinp= outinp1+ bout\n    output = sigmoid(outinp)\n\n#Backpropagation\n    EO = y-output\n    outgrad = derivatives_sigmoid(output)\n    d_output = EO* outgrad\n    EH = d_output.dot(wout.T)\n\n#how much hidden layer weights contributed to error\n    hiddengrad = derivatives_sigmoid(hlayer_act)\n    d_hiddenlayer = EH * hiddengrad","metadata":{"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# dotproduct of nextlayererror and currentlayerop\nwout += hlayer_act.T.dot(d_output) *lr\nwh += X.T.dot(d_hiddenlayer) *lr","metadata":{"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"print(\"Input: \\n\" + str(X)) \nprint(\"Actual Output: \\n\" + str(y))\nprint(\"Predicted Output: \\n\" ,output)","metadata":{"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Input: \n[[0.66666667 1.        ]\n [0.33333333 0.55555556]\n [1.         0.66666667]]\nActual Output: \n[[0.92]\n [0.86]\n [0.89]]\nPredicted Output: \n [[0.88840235]\n [0.87742047]\n [0.88856681]]\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}